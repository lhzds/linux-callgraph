mm/hugetlb.o
__alloc_bootmem_huge_page -> (hstate_next_node_to_alloc, memblock_alloc_try_nid_raw)
__nr_hugepages_store_common -> (alloc_pool_huge_page, mutex_lock, adjust_pool_surplus, remove_pool_huge_page, mutex_unlock, _raw_spin_lock_irq, flush_work, _raw_spin_unlock_irq, __SCT__cond_resched, init_nodemask_of_mempolicy, __update_and_free_hugetlb_folio, free_hpage_workfn)
__prep_compound_gigantic_folio -> (_printk)
__unmap_hugepage_range -> (tlb_flush_mmu, set_page_dirty, _raw_spin_unlock, _raw_spin_lock, huge_pmd_unshare, page_remove_rmap, tlb_flush_mmu_tlbonly, __tlb_remove_page_size)
__unmap_hugepage_range_final -> (down_write, refcount_warn_saturate, up_write, kfree, __unmap_hugepage_range)
__update_and_free_hugetlb_folio -> (free_contig_range, _raw_spin_lock_irq, _raw_spin_unlock_irq, hugetlb_vmemmap_restore, __free_pages)
__vma_reservation_common -> (region_add, _raw_spin_unlock, _raw_spin_lock, region_del, region_chg)
alloc_buddy_hugetlb_folio -> (_printk, __free_pages, __alloc_pages)
alloc_fresh_hugetlb_folio -> (alloc_buddy_hugetlb_folio, hugetlb_vmemmap_optimize, free_contig_range, _raw_spin_lock_irq, __prep_compound_gigantic_folio, _raw_spin_unlock_irq)
alloc_hugetlb_folio -> (_raw_spin_lock, hugetlb_cgroup_uncharge_folio_rsvd, hugetlb_cgroup_uncharge_cgroup_rsvd, hugetlb_cgroup_charge_cgroup_rsvd, huge_node, hugetlb_acct_memory, _raw_spin_unlock, _raw_spin_lock_irq, hugetlb_cgroup_commit_charge, dequeue_hugetlb_folio_nodemask, region_chg, hugetlb_cgroup_uncharge_cgroup, __mpol_put, hugetlb_cgroup_commit_charge_rsvd, _raw_spin_unlock_irq, alloc_surplus_hugetlb_folio, hugetlb_cgroup_charge_cgroup, region_add, hugepage_subpool_put_pages)
alloc_hugetlb_folio_nodemask -> (dequeue_hugetlb_folio_nodemask, alloc_fresh_hugetlb_folio, _raw_spin_lock_irq, _raw_spin_unlock_irq)
alloc_hugetlb_folio_vma -> (alloc_hugetlb_folio_nodemask, __mpol_put, huge_node)
alloc_pool_huge_page -> (free_huge_page, alloc_fresh_hugetlb_folio)
alloc_surplus_hugetlb_folio -> (free_huge_page, alloc_fresh_hugetlb_folio, _raw_spin_lock_irq, _raw_spin_unlock_irq)
allocate_file_region_entries -> (_raw_spin_lock, kfree, kmalloc_trace, _raw_spin_unlock)
clear_vma_resv_huge_pages -> (refcount_warn_saturate, region_del, __rcu_read_lock, __rcu_read_unlock, kfree)
copy_hugetlb_cgroup_uncharge_info -> (__rcu_read_unlock, __rcu_read_lock)
copy_hugetlb_page_range -> (__mmu_notifier_invalidate_range_start, __SCT__might_resched, __folio_put, _raw_spin_unlock, _raw_spin_lock, alloc_hugetlb_folio, down_read, up_read, __mmu_notifier_invalidate_range_end, huge_pte_alloc, copy_user_large_folio, restore_reserve_on_error, hugepage_add_new_anon_rmap)
default_hugepagesz_setup -> (_printk, hugetlb_add_hstate, hugetlb_hstate_alloc_pages, arch_hugetlb_valid_size, memparse)
demote_size_show -> (sysfs_emit)
demote_size_store -> (mutex_unlock, memparse, mutex_lock)
demote_store -> (_printk, hugetlb_vmemmap_optimize, mutex_lock, free_huge_page, mutex_unlock, _raw_spin_lock_irq, kstrtoull, _raw_spin_unlock_irq, hugetlb_vmemmap_restore, prep_compound_page)
dequeue_hugetlb_folio_nodemask -> (cpuset_node_allowed, __next_zones_zonelist)
dissolve_free_huge_page -> (_raw_spin_lock_irq, _raw_spin_unlock_irq, hugetlb_vmemmap_restore, __SCT__cond_resched, __update_and_free_hugetlb_folio)
dissolve_free_huge_pages -> (dissolve_free_huge_page)
folio_putback_active_hugetlb -> (_raw_spin_lock_irq, __folio_put, _raw_spin_unlock_irq)
follow_hugetlb_page -> (__folio_put, _raw_spin_unlock, _raw_spin_lock, down_read, up_read, _compound_head, __filemap_get_folio, try_grab_folio, hugetlb_fault)
free_hpage_workfn -> (__update_and_free_hugetlb_folio, __SCT__cond_resched)
free_huge_page -> (hugetlb_acct_memory, queue_work_on, _raw_spin_lock_irqsave, hugetlb_cgroup_uncharge_folio_rsvd, llist_add_batch, hugetlb_cgroup_uncharge_folio, _raw_spin_unlock_irqrestore, __update_and_free_hugetlb_folio, kfree, free_hpage_workfn)
free_hugepages_show -> (sysfs_emit)
gather_bootmem_prealloc -> (hugetlb_vmemmap_optimize, free_contig_range, free_huge_page, adjust_managed_page_count, _raw_spin_lock_irq, __prep_compound_gigantic_folio, _raw_spin_unlock_irq, __SCT__cond_resched)
get_huge_page_for_hwpoison -> (_raw_spin_lock_irq, _raw_spin_unlock_irq)
get_hwpoison_hugetlb_folio -> (_raw_spin_lock_irq, _raw_spin_unlock_irq)
huge_pmd_share -> (vma_interval_tree_iter_first, vma_mmu_pagesize, __folio_put, __pmd_alloc, _raw_spin_unlock, down_read, _raw_spin_lock, up_read, vma_interval_tree_iter_next)
huge_pmd_unshare -> (__folio_put)
huge_pte_alloc -> (huge_pmd_share, __p4d_alloc, __pud_alloc, __pmd_alloc)
hugepage_new_subpool -> (hugetlb_acct_memory, kfree, kmalloc_trace)
hugepage_put_subpool -> (hugetlb_acct_memory, _raw_spin_lock_irqsave, _raw_spin_unlock_irqrestore, kfree)
hugepage_subpool_put_pages -> (hugetlb_acct_memory, _raw_spin_lock_irqsave, _raw_spin_unlock_irqrestore, kfree)
hugepages_setup -> (_printk, hugetlb_hstate_alloc_pages, sscanf, hugetlb_node_alloc_supported)
hugepagesz_setup -> (_printk, hugetlb_add_hstate, memparse, arch_hugetlb_valid_size)
hugetlb_acct_memory -> (cpuset_nodemask_valid_mems_allowed, free_huge_page, _raw_spin_lock_irq, _raw_spin_unlock_irq, __SCT__cond_resched, get_task_policy, alloc_surplus_hugetlb_folio, __update_and_free_hugetlb_folio, apply_policy_zone, remove_pool_huge_page)
hugetlb_add_hstate -> (snprintf, __mutex_init)
hugetlb_add_to_page_cache -> (_raw_spin_lock, folio_mark_dirty, _raw_spin_unlock, __filemap_add_folio)
hugetlb_basepage_index -> (__page_file_index)
hugetlb_cgroup_put_rsvd_cgroup -> (__rcu_read_unlock, __rcu_read_lock)
hugetlb_change_protection -> (down_write, __mmu_notifier_invalidate_range_start, __SCT__might_resched, pfn_swap_entry_to_page, up_write, _raw_spin_unlock, _raw_spin_lock, huge_pmd_unshare, flush_tlb_mm_range, __mmu_notifier_invalidate_range_end, huge_pte_alloc)
hugetlb_fault -> (hugetlb_add_to_page_cache, __SCT__might_resched, mutex_lock, _raw_spin_lock, __anon_vma_prepare, __folio_put, _raw_spin_unlock, alloc_hugetlb_folio, __filemap_get_folio, vma_end_reservation, hugepage_add_new_anon_rmap, region_chg, _printk, __folio_lock, vma_needs_reservation, mutex_unlock, down_read, hugetlb_wp, folio_wait_bit, ___ratelimit, migration_entry_wait_huge, restore_reserve_on_error, clear_huge_page, up_read, huge_pte_alloc, folio_unlock, ptep_set_access_flags)
hugetlb_fix_reserve_counts -> (_printk, _raw_spin_lock_irq, hugetlb_acct_memory, _raw_spin_unlock_irq)
hugetlb_follow_page_mask -> (_raw_spin_unlock, down_read, _raw_spin_lock, up_read, try_grab_page)
hugetlb_hstate_alloc_pages -> (alloc_pool_huge_page, _printk, alloc_bootmem_huge_page, hugetlb_hstate_alloc_pages_onenode, string_get_size, kmalloc_trace, __SCT__cond_resched, kfree)
hugetlb_hstate_alloc_pages_onenode -> (_printk, alloc_bootmem_huge_page, free_huge_page, string_get_size, __SCT__cond_resched, alloc_fresh_hugetlb_folio)
hugetlb_init -> (_printk, hugetlb_add_hstate, __mutex_init, hugetlb_init_hstates, gather_bootmem_prealloc, string_get_size, hugetlb_mempolicy_sysctl_handler, proc_dointvec, __register_sysctl_init, hugetlb_overcommit_handler, hugetlb_sysfs_init, report_hugepages, hugetlb_sysctl_handler, hugetlb_cgroup_file_init, __kmalloc)
hugetlb_init_hstates -> (hugetlb_hstate_alloc_pages)
hugetlb_mempolicy_sysctl_handler -> (proc_doulongvec_minmax, __nr_hugepages_store_common)
hugetlb_overcommit_handler -> (proc_doulongvec_minmax, _raw_spin_lock_irq, _raw_spin_unlock_irq)
hugetlb_page_mapping_lock_write -> (down_write_trylock, page_mapping)
hugetlb_register_all_nodes -> (hugetlb_register_node)
hugetlb_register_node -> (kobject_create_and_add, _printk, nr_hugepages_store, hugetlb_unregister_node, free_hugepages_show, surplus_hugepages_show, hugetlb_sysfs_add_hstate, nr_hugepages_show)
hugetlb_report_meminfo -> (seq_printf)
hugetlb_report_node_meminfo -> (sysfs_emit_at)
hugetlb_report_usage -> (seq_printf)
hugetlb_reserve_pages -> (refcount_warn_saturate, _raw_spin_lock, hugetlb_cgroup_put_rsvd_cgroup, hugetlb_cgroup_uncharge_cgroup_rsvd, hugetlb_cgroup_charge_cgroup_rsvd, hugetlb_acct_memory, __init_rwsem, _raw_spin_unlock, _raw_spin_lock_irq, kmalloc_trace, resv_map_alloc, region_chg, _printk, _raw_spin_unlock_irq, region_del, down_write, region_add, up_write, hugepage_subpool_put_pages, kfree)
hugetlb_resv_map_add -> (__rcu_read_unlock, kfree, __rcu_read_lock)
hugetlb_show_meminfo_node -> (_printk)
hugetlb_sysctl_handler -> (proc_doulongvec_minmax, __nr_hugepages_store_common)
hugetlb_sysfs_add_hstate -> (kobject_create_and_add, _printk, sysfs_remove_group, demote_size_show, kobject_put, demote_size_store, sysfs_create_group, demote_store)
hugetlb_sysfs_init -> (kobject_create_and_add, _printk, nr_hugepages_mempolicy_show, resv_hugepages_show, nr_hugepages_store, free_hugepages_show, nr_overcommit_hugepages_show, nr_overcommit_hugepages_store, surplus_hugepages_show, hugetlb_sysfs_add_hstate, nr_hugepages_show, hugetlb_register_all_nodes, nr_hugepages_mempolicy_store)
hugetlb_unregister_node -> (nr_hugepages_store, sysfs_remove_group, free_hugepages_show, demote_size_show, surplus_hugepages_show, kobject_put, nr_hugepages_show, demote_size_store, demote_store)
hugetlb_unreserve_pages -> (hugetlb_acct_memory, _raw_spin_unlock, _raw_spin_lock, hugepage_subpool_put_pages, region_del)
hugetlb_unshare_all_pmds -> (hugetlb_unshare_pmds)
hugetlb_unshare_pmds -> (down_write, __mmu_notifier_invalidate_range_start, __SCT__might_resched, up_write, _raw_spin_unlock, _raw_spin_lock, huge_pmd_unshare, flush_tlb_mm_range, __mmu_notifier_invalidate_range_end)
hugetlb_vm_op_close -> (down_write, hugetlb_acct_memory, refcount_warn_saturate, up_write, _raw_spin_unlock, _raw_spin_lock, hugepage_subpool_put_pages, region_del, hugetlb_cgroup_uncharge_counter, kfree)
hugetlb_vm_op_open -> (_printk, refcount_warn_saturate, __init_rwsem, kmalloc_trace, __rcu_read_lock, __rcu_read_unlock)
hugetlb_vm_op_split -> (hugetlb_unshare_pmds)
hugetlb_vma_lock_read -> (down_read)
hugetlb_vma_lock_release -> (kfree)
hugetlb_vma_lock_write -> (down_write)
hugetlb_vma_trylock_write -> (down_write_trylock)
hugetlb_vma_unlock_read -> (up_read)
hugetlb_vma_unlock_write -> (up_write)
hugetlb_wp -> (vma_interval_tree_iter_first, __SCT__might_resched, mutex_lock, _raw_spin_lock, copy_user_large_folio, __delayacct_wpcopy_end, __folio_put, __anon_vma_prepare, _raw_spin_unlock, alloc_hugetlb_folio, hugepage_add_new_anon_rmap, __mmu_notifier_invalidate_range_start, ptep_clear_flush, mutex_unlock, down_read, __mmu_notifier_invalidate_range_end, vma_interval_tree_iter_next, __delayacct_wpcopy_start, restore_reserve_on_error, down_write, page_move_anon_rmap, up_write, unmap_hugepage_range, folio_total_mapcount, up_read, page_remove_rmap, __mmu_notifier_invalidate_range, ptep_set_access_flags)
isolate_hugetlb -> (_raw_spin_lock_irq, _raw_spin_unlock_irq)
isolate_or_dissolve_huge_page -> (alloc_buddy_hugetlb_folio, hugetlb_vmemmap_optimize, _raw_spin_lock_irq, _raw_spin_unlock_irq, __SCT__cond_resched, __update_and_free_hugetlb_folio)
move_hugetlb_page_tables -> (down_write, __mmu_notifier_invalidate_range_start, __SCT__might_resched, up_write, _raw_spin_unlock, _raw_spin_lock, huge_pmd_unshare, flush_tlb_mm_range, __mmu_notifier_invalidate_range_end, huge_pte_alloc)
move_hugetlb_state -> (hugetlb_cgroup_migrate, _raw_spin_lock_irq, _raw_spin_unlock_irq)
nr_hugepages_mempolicy_show -> (sysfs_emit)
nr_hugepages_mempolicy_store -> (kstrtoull, __nr_hugepages_store_common)
nr_hugepages_show -> (sysfs_emit)
nr_hugepages_store -> (kstrtoull, __nr_hugepages_store_common)
nr_overcommit_hugepages_show -> (sysfs_emit)
nr_overcommit_hugepages_store -> (_raw_spin_lock_irq, kstrtoull, _raw_spin_unlock_irq)
region_add -> (_raw_spin_lock, allocate_file_region_entries, hugetlb_resv_map_add, _raw_spin_unlock)
region_chg -> (_raw_spin_lock, allocate_file_region_entries, hugetlb_resv_map_add, _raw_spin_unlock)
region_del -> (hugetlb_cgroup_uncharge_file_region, _raw_spin_unlock, _raw_spin_lock, kmalloc_trace, kfree, copy_hugetlb_cgroup_uncharge_info)
report_hugepages -> (_printk, string_get_size)
restore_reserve_on_error -> (__vma_reservation_common, region_chg, _raw_spin_lock, _raw_spin_unlock)
resv_hugepages_show -> (sysfs_emit)
resv_map_alloc -> (kmalloc_trace, kfree)
resv_map_release -> (region_del, kfree)
surplus_hugepages_show -> (sysfs_emit)
tlb_flush_mmu_tlbonly -> (flush_tlb_mm_range, __mmu_notifier_invalidate_range)
unmap_hugepage_range -> (tlb_gather_mmu, __mmu_notifier_invalidate_range_start, __SCT__might_resched, __mmu_notifier_invalidate_range_end, tlb_finish_mmu, __unmap_hugepage_range)
vma_end_reservation -> (_raw_spin_lock, _raw_spin_unlock)
vma_needs_reservation -> (region_chg)
