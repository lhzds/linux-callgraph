kernel/sched/fair.o
__pick_next_task_fair -> (set_next_entity, pick_next_entity, update_misfit_status)
__sched_group_set_shares -> (update_load_avg, reweight_entity, update_rq_clock, raw_spin_rq_unlock, raw_spin_rq_lock_nested)
__update_idle_core -> (__rcu_read_unlock, __rcu_read_lock, available_idle_cpu)
_nohz_idle_balance -> (rebalance_domains, _find_next_bit, _find_first_bit, update_blocked_averages, update_rq_clock, idle_cpu, raw_spin_rq_unlock, raw_spin_rq_lock_nested)
active_load_balance_cpu_stop -> (check_preempt_curr, set_task_cpu, activate_task, __rcu_read_lock, update_rq_clock, deactivate_task, __rcu_read_unlock, raw_spin_rq_unlock, can_migrate_task, raw_spin_rq_lock_nested)
alloc_fair_sched_group -> (kfree, kmalloc_node_trace, __kmalloc)
attach_entity_load_avg -> (__probestub_pelt_cfs_tp, __traceiter_pelt_cfs_tp, __SCT__tp_func_pelt_cfs_tp)
attach_task_cfs_rq -> (attach_entity_load_avg, update_load_avg, propagate_entity_cfs_rq)
balance_fair -> (newidle_balance)
can_migrate_task -> (get_rr_interval_fair, check_preempt_wakeup, task_fork_fair, rq_online_fair, migrate_task_rq_fair, enqueue_task_fair, prio_changed_fair, task_tick_fair, set_next_task_fair, yield_to_task_fair, update_curr_fair, dequeue_task_fair, put_prev_task_fair, set_cpus_allowed_common, rq_offline_fair, task_dead_fair, switched_to_fair, select_task_rq_fair, switched_from_fair, __pick_next_task_fair, yield_task_fair, balance_fair, task_change_group_fair, pick_task_fair, kthread_is_per_cpu)
check_preempt_wakeup -> (update_curr, resched_curr)
dequeue_task_fair -> (update_curr, update_load_avg, reweight_entity, __update_stats_wait_end, __SCT__tp_func_sched_update_nr_running_tp, rb_next, __probestub_sched_update_nr_running_tp, call_trace_sched_update_nr_running, __SCT__tp_func_sched_util_est_se_tp, __probestub_sched_util_est_se_tp, __traceiter_sched_util_est_cfs_tp, __traceiter_sched_update_nr_running_tp, __SCT__tp_func_sched_util_est_cfs_tp, rb_erase, __probestub_sched_util_est_cfs_tp, __traceiter_sched_util_est_se_tp)
detach_entity_load_avg -> (__probestub_pelt_cfs_tp, __traceiter_pelt_cfs_tp, __SCT__tp_func_pelt_cfs_tp)
detach_task_cfs_rq -> (update_load_avg, propagate_entity_cfs_rq, detach_entity_load_avg)
enqueue_task_fair -> (__traceiter_sched_stat_wait, __SCT__tp_func_sched_overutilized_tp, __SCT__tp_func_sched_stat_runtime, call_trace_sched_update_nr_running, __SCT__tp_func_sched_stat_blocked, _printk_deferred, __probestub_sched_stat_wait, __traceiter_sched_stat_blocked, __traceiter_sched_util_est_cfs_tp, __SCT__tp_func_sched_util_est_cfs_tp, __traceiter_sched_overutilized_tp, reweight_entity, __SCT__tp_func_sched_stat_sleep, __probestub_sched_stat_runtime, __probestub_sched_update_nr_running_tp, __update_stats_wait_start, __probestub_sched_overutilized_tp, __SCT__tp_func_sched_stat_iowait, __SCT__tp_func_sched_stat_wait, update_curr, update_load_avg, __traceiter_sched_stat_sleep, __update_stats_enqueue_sleeper, __traceiter_sched_stat_runtime, __probestub_sched_stat_blocked, __traceiter_sched_update_nr_running_tp, __probestub_sched_util_est_cfs_tp, rb_insert_color, __SCT__tp_func_sched_update_nr_running_tp, __traceiter_sched_stat_iowait, __probestub_sched_stat_iowait, __probestub_sched_stat_sleep)
find_idlest_cpu -> (__update_load_avg_blocked_se, available_idle_cpu, cpu_util)
free_fair_sched_group -> (kfree)
get_rr_interval_fair -> (sched_slice)
init_sched_fair_class -> (open_softirq, run_rebalance_domains)
load_balance -> (check_preempt_curr, update_group_capacity, __traceiter_sched_overutilized_tp, __SCT__tp_func_sched_overutilized_tp, active_load_balance_cpu_stop, need_active_balance, arch_asym_cpu_priority, set_task_cpu, activate_task, stop_one_cpu_nowait, update_rq_clock, deactivate_task, group_balance_cpu, __probestub_sched_overutilized_tp, raw_spin_rq_unlock, can_migrate_task, idle_cpu, raw_spin_rq_lock_nested)
migrate_task_rq_fair -> (_raw_spin_lock_irqsave, __update_load_avg_blocked_se, sched_clock_cpu, __rcu_read_lock, _raw_spin_unlock_irqrestore, __rcu_read_unlock)
need_active_balance -> (arch_asym_cpu_priority, idle_cpu)
newidle_balance -> (load_balance, sched_clock_cpu, housekeeping_test_cpu, update_blocked_averages, raw_spin_rq_unlock, __rcu_read_lock, __rcu_read_unlock, __msecs_to_jiffies, raw_spin_rq_lock_nested)
nohz_balance_enter_idle -> (__rcu_read_unlock, __rcu_read_lock, housekeeping_test_cpu)
nohz_balance_exit_idle -> (set_cpu_sd_state_busy)
nohz_run_idle_balance -> (_nohz_idle_balance)
online_fair_sched_group -> (update_load_avg, attach_entity_load_avg, propagate_entity_cfs_rq, update_rq_clock, raw_spin_rq_unlock, raw_spin_rq_lock_nested)
pick_next_entity -> (rb_next)
pick_next_task_fair -> (get_rr_interval_fair, check_preempt_wakeup, newidle_balance, task_fork_fair, rq_online_fair, migrate_task_rq_fair, enqueue_task_fair, prio_changed_fair, task_tick_fair, set_next_task_fair, pick_next_entity, yield_to_task_fair, update_curr, update_curr_fair, dequeue_task_fair, put_prev_task_fair, set_cpus_allowed_common, rq_offline_fair, task_dead_fair, switched_to_fair, update_misfit_status, select_task_rq_fair, switched_from_fair, __pick_next_task_fair, put_prev_entity, yield_task_fair, balance_fair, task_change_group_fair, set_next_entity, pick_task_fair)
pick_task_fair -> (update_curr, pick_next_entity)
place_entity -> (sched_slice)
post_init_entity_util_avg -> (get_rr_interval_fair, check_preempt_wakeup, task_fork_fair, rq_online_fair, migrate_task_rq_fair, enqueue_task_fair, prio_changed_fair, task_tick_fair, set_next_task_fair, yield_to_task_fair, update_curr_fair, dequeue_task_fair, put_prev_task_fair, set_cpus_allowed_common, rq_offline_fair, task_dead_fair, switched_to_fair, select_task_rq_fair, switched_from_fair, __pick_next_task_fair, yield_task_fair, balance_fair, task_change_group_fair, pick_task_fair)
prio_changed_fair -> (check_preempt_curr, resched_curr)
propagate_entity_cfs_rq -> (update_load_avg)
put_prev_entity -> (update_load_avg, __update_stats_wait_start, rb_insert_color, update_curr)
put_prev_task_fair -> (put_prev_entity)
rebalance_domains -> (load_balance, _raw_spin_unlock, _raw_spin_trylock, __rcu_read_lock, idle_cpu, __rcu_read_unlock, __msecs_to_jiffies)
reweight_entity -> (update_curr)
reweight_task -> (reweight_entity)
run_rebalance_domains -> (update_blocked_averages, rebalance_domains, _nohz_idle_balance)
sched_fair_sysctl_init -> (__register_sysctl_init, proc_dointvec)
sched_group_set_idle -> (mutex_lock, mutex_unlock, raw_spin_rq_unlock, __sched_group_set_shares, raw_spin_rq_lock_nested)
sched_group_set_shares -> (mutex_unlock, mutex_lock, __sched_group_set_shares)
sched_init_granularity -> (update_sysctl)
select_task_rq_fair -> (cpus_share_cache, __update_load_avg_blocked_se, _find_first_bit, __rcu_read_lock, find_idlest_cpu, available_idle_cpu, __rcu_read_unlock, _find_next_bit)
set_cpu_sd_state_busy -> (__rcu_read_unlock, __rcu_read_lock)
set_next_entity -> (update_load_avg, __update_stats_wait_end, rb_erase, rb_next)
set_next_task_fair -> (set_next_entity)
set_task_rq_fair -> (__update_load_avg_blocked_se)
setup_sched_thermal_decay_shift -> (kstrtoint, _printk)
switched_from_fair -> (detach_task_cfs_rq)
switched_to_fair -> (check_preempt_curr, attach_task_cfs_rq, resched_curr)
task_change_group_fair -> (attach_task_cfs_rq, detach_task_cfs_rq)
task_dead_fair -> (_raw_spin_lock_irqsave, __update_load_avg_blocked_se, _raw_spin_unlock_irqrestore)
task_fork_fair -> (place_entity, update_curr, resched_curr, update_rq_clock, raw_spin_rq_unlock, raw_spin_rq_lock_nested)
task_tick_fair -> (sched_slice, update_curr, update_load_avg, __SCT__tp_func_sched_overutilized_tp, reweight_entity, __traceiter_sched_overutilized_tp, resched_curr, update_misfit_status, __probestub_sched_overutilized_tp, hrtimer_active)
trigger_load_balance -> (smp_call_function_single_async, arch_asym_cpu_priority, __rcu_read_lock, idle_cpu, __rcu_read_unlock, raise_softirq, housekeeping_cpumask)
unregister_fair_sched_group -> (_raw_spin_lock_irqsave, __update_load_avg_blocked_se, _raw_spin_unlock_irqrestore, raw_spin_rq_unlock, raw_spin_rq_lock_nested)
update_blocked_averages -> (pick_task_dl, _raw_spin_lock, enqueue_task_rt, rq_offline_dl, update_rq_clock, balance_rt, __update_load_avg_cfs_rq, find_lock_lowest_rq, raw_spin_rq_unlock, task_tick_dl, switched_to_dl, update_curr_rt, pick_next_task_rt, balance_dl, _raw_spin_unlock, find_lock_later_rq, dequeue_task_rt, put_prev_task_dl, set_next_task_rt, pick_task_rt, task_woken_rt, update_curr_dl, set_next_task_dl, yield_task_rt, select_task_rq_rt, update_load_avg, update_dl_rq_load_avg, switched_from_dl, migrate_task_rq_dl, switched_from_rt, check_preempt_curr_rt, prio_changed_rt, yield_task_dl, prio_changed_dl, set_cpus_allowed_dl, get_rr_interval_rt, set_cpus_allowed_common, task_tick_rt, rq_offline_rt, enqueue_task_dl, dequeue_task_dl, put_prev_task_rt, switched_to_rt, select_task_rq_dl, task_fork_dl, update_rt_rq_load_avg, check_preempt_curr_dl, task_woken_dl, rq_online_dl, rq_online_rt, pick_next_task_dl, raw_spin_rq_lock_nested)
update_curr -> (__SCT__tp_func_sched_stat_runtime, cpuacct_charge, __probestub_sched_stat_runtime, __traceiter_sched_stat_runtime, __cgroup_account_cputime)
update_curr_fair -> (update_curr)
update_group_capacity -> (__traceiter_sched_cpu_capacity_tp, __probestub_sched_cpu_capacity_tp, __SCT__tp_func_sched_cpu_capacity_tp, __msecs_to_jiffies)
update_load_avg -> (__SCT__tp_func_pelt_cfs_tp, _raw_spin_unlock, _raw_spin_lock, __SCT__tp_func_pelt_se_tp, attach_entity_load_avg, detach_entity_load_avg, __traceiter_pelt_se_tp, __probestub_pelt_se_tp, __probestub_pelt_cfs_tp, __traceiter_pelt_cfs_tp, __update_load_avg_cfs_rq, __update_load_avg_se)
yield_task_fair -> (update_curr, update_rq_clock)
yield_to_task_fair -> (yield_task_fair)
